{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div>\n",
    "    <p style=\"float: right;\"><img width=\"66%\" src=\"templates/logo_fmriflows.gif\"></p>\n",
    "    <h1>Anatomical Preprocessing</h1>\n",
    "    <p>This notebooks preprocesses anatomical MRI images by executing the following processing steps:\n",
    "\n",
    "1. Reorient images to RAS\n",
    "1. Crop FOV with FSL\n",
    "1. N4-inhomogenity correction with ANTS\n",
    "1. GM, WM and CSF segmentation with SPM\n",
    "1. Brainmask creation and brain extraction with Nilearn\n",
    "1. Normalization to ICBM template with ANTS</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Structure Requirements\n",
    "\n",
    "The data structure to run this notebook should be according to the BIDS format:\n",
    "\n",
    "    dataset\n",
    "    ├── fmriflows_spec_preproc.json\n",
    "    └── sub-{sub_id}\n",
    "        └── anat\n",
    "            └── sub-{sub_id}_{T1w_id}.nii.gz\n",
    "            \n",
    "**Note:** Subfolders for individual scan sessions are optional.\n",
    "\n",
    "`fmriflows` will run the preprocessing on all files of a subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Execution Specifications\n",
    "\n",
    "This notebook will extract the relevant processing specifications from the `fmriflows_spec_preproc.json` file in the dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join as opj\n",
    "\n",
    "spec_file = opj('/data', 'fmriflows_spec_preproc.json')\n",
    "\n",
    "with open(spec_file) as f:\n",
    "    specs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters for anatomical preprocessing workflow\n",
    "subject_list = specs['subject_list_anat']\n",
    "session_list = specs['session_list_anat']\n",
    "T1w_id = specs['T1w_id']\n",
    "res_norm = specs['res_norm']\n",
    "norm_accuracy = specs['norm_accuracy']\n",
    "n_proc = specs['n_parallel_jobs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to change any of those values manually, overwrite them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subject identifiers\n",
    "subject_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of session identifiers\n",
    "session_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anatomical image identifier\n",
    "T1w_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolution of normalized images\n",
    "res_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANTs Normalization accuracy\n",
    "norm_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parallel jobs to run\n",
    "n_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Workflow\n",
    "\n",
    "To ensure a good overview of the anatomical preprocessing, the workflow was divided into two subworkflows:\n",
    "\n",
    "1. The Main Workflow, i.e. doing the actual preprocessing\n",
    "2. Report Workflow, i.e. visualizating relevant steps for quality control\n",
    "\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as opj\n",
    "from nipype import Node, Workflow, Function, IdentityInterface\n",
    "from nipype.interfaces.image import Reorient\n",
    "from nipype.interfaces.fsl import RobustFOV\n",
    "from nipype.interfaces.ants import N4BiasFieldCorrection, Registration\n",
    "from nipype.algorithms.misc import Gunzip\n",
    "from nipype.interfaces.spm import NewSegment\n",
    "from nipype.interfaces.io import SelectFiles, DataSink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify SPM location\n",
    "from nipype.interfaces.matlab import MatlabCommand\n",
    "MatlabCommand.set_default_paths('/opt/spm12-r7219/spm12_mcr/spm12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Execution Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths and names\n",
    "exp_dir = '/data/derivatives'\n",
    "out_dir = 'fmriflows'\n",
    "work_dir = '/workingdir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fmriflows output folder if missing\n",
    "import pathlib\n",
    "pathlib.Path(opj(exp_dir, out_dir)).mkdir(parents=True, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of template brain with desired voxel resolution\n",
    "template_dir = '/templates/mni_icbm152_nlin_asym_09c/'\n",
    "brain_template = opj(template_dir, '1.0mm_brain.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample template brain to desired resolution\n",
    "from nibabel import load, Nifti1Image\n",
    "from nilearn.image import resample_img\n",
    "from nibabel.spaces import vox2out_vox\n",
    "\n",
    "img = load(brain_template)\n",
    "target_shape, target_affine = vox2out_vox(img, voxel_sizes=res_norm)\n",
    "img_resample = resample_img(img, target_affine, target_shape, clip=True)\n",
    "norm_template = opj(template_dir, 'template_brain_%s.nii.gz' %'_'.join([str(n) for n in res_norm]))\n",
    "img_resample.to_filename(norm_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subworkflow for the Main Workflow\n",
    "\n",
    "### Implement Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorient anatomical images to RAS\n",
    "reorient = Node(Reorient(orientation='RAS'), name='reorient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduces FOV of images to remove lower head and neck\n",
    "crop_FOV = Node(RobustFOV(output_type='NIFTI_GZ'), name='crop_FOV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrects bias field\n",
    "n4 = Node(N4BiasFieldCorrection(dimension=3), name='n4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gunzips images\n",
    "gunzip = Node(Gunzip(), name='gunzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segments brain into 5 classes (GM, WM, CSF, Skull & Head)\n",
    "segment = Node(NewSegment(), name='segment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Brain Mask and Extract Brain\n",
    "def get_brain_and_mask(in_file, segments):\n",
    "    \n",
    "    import nibabel as nb\n",
    "    from nilearn.image import clean_img, mean_img, math_img\n",
    "    from scipy.ndimage.morphology import (\n",
    "        binary_fill_holes, binary_dilation, binary_erosion)\n",
    "    from os.path import basename, abspath\n",
    "\n",
    "    # Load T1w corrected image\n",
    "    img = nb.load(in_file)\n",
    "\n",
    "    # Brainmask is created from the probability tissue maps\n",
    "    gm, wm, csf, skull, head = [s[0] for s in segments]\n",
    "    img_gmwm = math_img(\"(img1 + img2) >= 0.25\", img1=gm, img2=wm)\n",
    "    img_csf = math_img(\"img1 >= 1.0\", img1=csf)\n",
    "    img_not_rest = math_img(\"(img1 + img2) >= 0.25\", img1=head, img2=skull)\n",
    "    img_mask = math_img(\"(img1 + img2 - img3) >= 1.0\", img1=img_gmwm, img2=img_csf, img3=img_not_rest)\n",
    "\n",
    "    # Improves brainmask by 1 x erosion, 2 x dilation & filling of wholes\n",
    "    data_mask = binary_erosion(\n",
    "                binary_fill_holes(\n",
    "                binary_dilation(\n",
    "                img_mask.get_data(),\n",
    "                    iterations = 2)),\n",
    "                    iterations = 1).astype('int8')\n",
    "    img_mask = nb.Nifti1Image(data_mask, img.affine, img.header)\n",
    "\n",
    "    # Extract Brain with Mask\n",
    "    img_brain = math_img(\"img1 * img2\", img1=img, img2=img_mask)\n",
    "\n",
    "    # Store output in nifti files\n",
    "    filename = abspath(basename(in_file))\n",
    "    out_file = filename.replace('.nii', '_brain.nii')\n",
    "    mask = filename.replace('.nii', '_brainmask.nii')\n",
    "    img_brain.to_filename(out_file)\n",
    "    img_mask.to_filename(mask)\n",
    "\n",
    "    return out_file, mask\n",
    "\n",
    "extract_brain = Node(Function(input_names=['in_file', 'segments'],\n",
    "                              output_names=['out_file', 'mask'],\n",
    "                              function=get_brain_and_mask),\n",
    "                     name='extract_brain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress segmentation files\n",
    "def compress_segments(segments):\n",
    "\n",
    "    import nibabel as nb\n",
    "    from os.path import basename, abspath\n",
    "    \n",
    "    # Change the compression level of the NIfTI image\n",
    "    nb.openers.Opener.default_compresslevel = 6\n",
    "\n",
    "    # Go through the individual segments\n",
    "    compressed_segments = []\n",
    "    for s in segments:\n",
    "        new_fname  = abspath(basename(s[0] + '.gz'))\n",
    "        nb.load(s[0]).to_filename(new_fname)\n",
    "        compressed_segments.append(new_fname)\n",
    "    \n",
    "    return compressed_segments\n",
    "\n",
    "compressor = Node(Function(input_names=['segments'],\n",
    "                           output_names=['compressed_segments'],\n",
    "                           function=compress_segments),\n",
    "                  name='compressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameters for ANTs normalization\n",
    "if norm_accuracy == 'precise':\n",
    "    par = {'metric': ['Mattes', 'Mattes', 'CC'],\n",
    "           'radius_or_number_of_bins': [56, 56, 4],\n",
    "           'transform_parameters': [[0.05], [0.08], [0.1, 3.0, 0.0]],\n",
    "           'number_of_iterations': [[200, 100], [200, 100], [100, 70, 50, 20]],\n",
    "           'sampling_strategy': ['Regular', 'Regular', 'None'],\n",
    "           'sampling_percentage': [0.25, 0.25, 1.0],\n",
    "           'smoothing_sigmas': [[2, 1], [1, 0], [3, 2, 1, 0]],\n",
    "           'shrink_factors': [[2, 1], [2, 1], [8, 4, 2, 1]]\n",
    "          }\n",
    "\n",
    "elif norm_accuracy == 'fast':\n",
    "    par = {'metric': ['Mattes', 'Mattes', 'Mattes'],\n",
    "           'radius_or_number_of_bins': [32, 32, 56],\n",
    "           'transform_parameters': [[0.01], [0.08], [0.1, 3.0, 0.0]],\n",
    "           'number_of_iterations': [[1000], [500, 250, 100], [50, 20]],\n",
    "           'sampling_strategy': ['Random', 'Regular', 'Regular'],\n",
    "           'sampling_percentage': [ 0.15, 0.15, 0.25],\n",
    "           'smoothing_sigmas': [[4], [4, 2, 0], [1, 0]],\n",
    "           'shrink_factors': [[4], [4, 2, 1], [2, 1]]\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize anatomy to ICBM template\n",
    "antsreg = Node(Registration(fixed_image=norm_template,\n",
    "                            num_threads=n_proc,\n",
    "                            output_inverse_warped_image=True,\n",
    "                            output_warped_image=True,\n",
    "                            collapse_output_transforms=True,\n",
    "                            dimension=3,\n",
    "                            initial_moving_transform_com=True,\n",
    "                            winsorize_lower_quantile=0.005,\n",
    "                            winsorize_upper_quantile=0.995,\n",
    "                            write_composite_transform=True,\n",
    "\n",
    "                            float=False,\n",
    "                            interpolation='BSpline',\n",
    "                            transforms=['Rigid', 'Affine', 'SyN'],\n",
    "                            sigma_units=['vox'] * 3,\n",
    "                            metric_weight=[1.0] * 3,\n",
    "                            use_estimate_learning_rate_once=[True] * 3,\n",
    "                            use_histogram_matching=True,\n",
    "\n",
    "                            radius_or_number_of_bins=par['radius_or_number_of_bins'],\n",
    "                            sampling_percentage=par['sampling_percentage'],\n",
    "                            sampling_strategy=par['sampling_strategy'],\n",
    "                            transform_parameters=par['transform_parameters'],\n",
    "                            metric=par['metric'],\n",
    "                            number_of_iterations=par['number_of_iterations'],\n",
    "                            convergence_threshold=[1e-06, 1e-06, 1e-06],\n",
    "                            convergence_window_size=[20, 20, 10],\n",
    "                            smoothing_sigmas=par['smoothing_sigmas'],\n",
    "                            shrink_factors=par['shrink_factors'],\n",
    "\n",
    "                            verbose=True,\n",
    "                            terminal_output='file'),\n",
    "               name='antsreg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Main Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create main preprocessing workflow\n",
    "mainflow = Workflow(name='mainflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to workflow and connect them\n",
    "mainflow.connect([(reorient, crop_FOV, [('out_file', 'in_file')]),\n",
    "                  (crop_FOV, n4, [('out_roi', 'input_image')]),\n",
    "                  (n4, gunzip, [('output_image', 'in_file')]),\n",
    "                  (gunzip, segment, [('out_file', 'channel_files')]),\n",
    "                  (segment, extract_brain, [('native_class_images', 'segments')]),\n",
    "                  (segment, compressor, [('native_class_images', 'segments')]),\n",
    "                  (n4, extract_brain, [('output_image', 'in_file')]),\n",
    "                  (extract_brain, antsreg, [('out_file', 'moving_image')])\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subworkflow for the report Workflow\n",
    "\n",
    "### Implement Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visual figures for anatomical preprocessing\n",
    "def plot_figures(sub, sess, n4, segments, brain, T1_template, norm_template, warped_file):\n",
    "    \n",
    "    import nibabel as nb\n",
    "    from nilearn.plotting import plot_stat_map, plot_roi\n",
    "    from nilearn.masking import apply_mask, unmask\n",
    "    from matplotlib.pyplot import figure\n",
    "\n",
    "    import numpy as np\n",
    "    from nilearn.image import math_img, smooth_img\n",
    "    from nilearn.plotting import find_cut_slices\n",
    "    from os.path import basename, abspath\n",
    "    \n",
    "    title_txt = 'sub: %s' % sub\n",
    "    \n",
    "    # Add session suffix if present\n",
    "    if sess:\n",
    "        title_txt += ' - sess: %s' % sess\n",
    "    \n",
    "    # Visualize Tissue Segmentation of T1w\n",
    "    img = nb.load(brain)\n",
    "    data = np.stack((np.zeros(img.shape),\n",
    "                     nb.load(segments[0][0]).get_data(),\n",
    "                     nb.load(segments[1][0]).get_data(),\n",
    "                     nb.load(segments[2][0]).get_data(),\n",
    "                     nb.load(segments[3][0]).get_data(),\n",
    "                     nb.load(segments[4][0]).get_data()), axis= -1)\n",
    "    label_id = np.argmax(data, axis=-1)\n",
    "    segmentation = nb.Nifti1Image(label_id, img.affine, img.header)\n",
    "\n",
    "    fig = figure(figsize=(16, 8))\n",
    "    for i, e in enumerate(['x', 'y', 'z']):\n",
    "        ax = fig.add_subplot(3, 1, i + 1)\n",
    "        \n",
    "        cuts = find_cut_slices(segmentation, direction=e, n_cuts=12)[2:-2]\n",
    "        plot_roi(segmentation, cmap='Accent', dim=1, annotate=False, bg_img=n4,\n",
    "                 display_mode=e, title=title_txt + ' - %s-axis' % e,\n",
    "                 resampling_interpolation='nearest', cut_coords=cuts, axes=ax)\n",
    "    \n",
    "    out_segmentation = basename(brain).replace('brain.nii.gz', 'segmentation.png')\n",
    "    fig.savefig(out_segmentation, bbox_inches='tight', facecolor='black',\n",
    "                frameon=True, dpi=300, transparent=False)\n",
    "\n",
    "    # Visualize Brain Extraction of T1w\n",
    "    fig = figure(figsize=(16, 8))\n",
    "    for i, e in enumerate(['x', 'y', 'z']):\n",
    "        ax = fig.add_subplot(3, 1, i + 1)\n",
    "        cuts = find_cut_slices(brain, direction=e, n_cuts=12)[2:-2]\n",
    "        plot_stat_map(brain, title=title_txt + ' - %s-axis' % e, colorbar=False,\n",
    "                      threshold='auto', bg_img=n4, cmap='magma', display_mode=e,\n",
    "                      resampling_interpolation='nearest', dim=-1,\n",
    "                      cut_coords=cuts, annotate=False, axes=ax)\n",
    "\n",
    "    out_brain = basename(brain).replace('.nii.gz', '.png')\n",
    "    fig.savefig(out_brain, bbox_inches='tight', facecolor='black', frameon=True,\n",
    "                dpi=300, transparent=False)\n",
    "    \n",
    "    # Visualize T1w to MNI registration and deformation differences\n",
    "\n",
    "    ## Smooth images\n",
    "    img_brain = smooth_img(norm_template, 2)\n",
    "    img_warp = smooth_img(warped_file, 2)\n",
    "\n",
    "    # Mask images\n",
    "    img_mask = math_img('img!=0', img=norm_template)\n",
    "    data_brain = apply_mask(img_brain, img_mask)\n",
    "    data_warp = apply_mask(img_warp, img_mask)\n",
    "\n",
    "    # Remove very small values\n",
    "    data_brain[data_brain<=1e-1] = 0\n",
    "    data_warp[data_warp<=1e-1] = 0\n",
    "\n",
    "    # Find most present value in the upper data value histogram\n",
    "    freq, val = np.histogram(data_brain,bins=64)\n",
    "    divider_brain = val[32+np.argmax(freq[32:])]\n",
    "\n",
    "    freq, val = np.histogram(data_warp,bins=64)\n",
    "    divider_warp = val[32+np.argmax(freq[32:])]\n",
    "\n",
    "    # 'Equalize' their histogram\n",
    "    img_brain = unmask(data_brain / divider_brain, img_mask)\n",
    "    img_warp = unmask(data_warp / divider_warp, img_mask)\n",
    "\n",
    "    ## Compute difference between warped image and brain template\n",
    "    img_dif = math_img('(img1 - img2) * 10', img1=img_brain, img2=img_warp)\n",
    "\n",
    "    fig = figure(figsize=(16, 8))\n",
    "    for i, e in enumerate(['x', 'y', 'z']):\n",
    "        ax = fig.add_subplot(3, 1, i + 1)\n",
    "        cuts = find_cut_slices(img_dif, direction=e, n_cuts=12)[2:-2]\n",
    "        plot_stat_map(img_dif, title=title_txt + ' - %s-axis' % e, colorbar=False,\n",
    "                        threshold=0, bg_img=T1_template, display_mode=e, alpha=0.8,\n",
    "                        resampling_interpolation='nearest', annotate=False, \n",
    "                        cmap='Spectral', cut_coords=cuts, axes=ax)\n",
    "    \n",
    "    out_warp = basename(warped_file).replace('.nii.gz', '.png')\n",
    "    fig.savefig(out_warp, bbox_inches='tight', facecolor='black', frameon=True,\n",
    "                dpi=300, transparent=False)\n",
    "    \n",
    "    return abspath(out_segmentation), abspath(out_brain), abspath(out_warp), sub, sess\n",
    "    \n",
    "# Create Plotting Node\n",
    "create_figures = Node(Function(input_names=['sub', 'sess', 'n4', 'segments', 'brain',\n",
    "                                           'T1_template', 'norm_template', 'warped_file'],\n",
    "                              output_names=['out_segmentation', 'out_brain', 'out_warp',\n",
    "                                            'sub', 'sess'],\n",
    "                              function=plot_figures),\n",
    "                name='create_figures')\n",
    "create_figures.inputs.norm_template = norm_template\n",
    "create_figures.inputs.T1_template = brain_template.replace('brain', 'T1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the HTML report\n",
    "def write_report(sub, sess, brain):\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    with open('/reports/report_template_preproc_anat.html', 'r') as report:\n",
    "        txt = report.read()\n",
    "        txt = txt.replace('sub-placeholder', 'sub-%s' % sub)\n",
    "        \n",
    "        # Add session suffix if present\n",
    "        if sess:\n",
    "            txt = txt.replace('ses-placeholder', 'ses-%s' % sess)\n",
    "            filename = 'sub-%s_ses-%s.html' % (sub, sess)\n",
    "        else:\n",
    "            txt = txt.replace('ses-placeholder', '')\n",
    "            txt = txt.replace('__', '_')\n",
    "            filename = 'sub-%s.html' % sub\n",
    "\n",
    "    report_file = os.path.join('/data', 'derivatives', 'fmriflows', filename)\n",
    "    \n",
    "    with open(report_file, 'w') as report:\n",
    "        report.writelines(txt)\n",
    "\n",
    "# Create Report Node\n",
    "create_report = Node(Function(input_names=['sub', 'sess', 'brain'],\n",
    "                              function=write_report),\n",
    "                     name='create_report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create report Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create report workflow\n",
    "reportflow = Workflow(name='reportflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to workflow and connect them\n",
    "reportflow.connect([(create_figures, create_report, [('sub', 'sub'),\n",
    "                                                     ('sess', 'sess'),\n",
    "                                                     ('out_brain', 'brain')\n",
    "                                                     ])\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Input & Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over subject and session id\n",
    "info_source = Node(IdentityInterface(fields=['subject_id', 'session_id']),\n",
    "                   name='info_source')\n",
    "\n",
    "iter_list = [('subject_id', subject_list)]\n",
    "\n",
    "if session_list:\n",
    "    iter_list.append(('session_id', session_list))\n",
    "else:\n",
    "    info_source.inputs.session_id = ''\n",
    "\n",
    "info_source.iterables = iter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path to input files\n",
    "def create_file_path(subject_id, session_id, T1w_id):\n",
    "\n",
    "    # Get all anatomical files\n",
    "    from bids.layout import BIDSLayout\n",
    "    layout = BIDSLayout('/data/')\n",
    "\n",
    "    search_parameters = {'datatype': 'anat',\n",
    "                         'return_type': 'file',\n",
    "                         'suffix': T1w_id,\n",
    "                         'subject': subject_id,\n",
    "                         'extensions': 'nii.gz',\n",
    "                        }\n",
    "    if session_id:\n",
    "        search_parameters['session'] = session_id\n",
    "\n",
    "    return layout.get(**search_parameters)[0]\n",
    "\n",
    "select_files = Node(Function(input_names=['subject_id', 'session_id',\n",
    "                                          'layout', 'T1w_id'],\n",
    "                             output_names=['anat'],\n",
    "                             function=create_file_path),\n",
    "                    name='select_files')\n",
    "select_files.inputs.T1w_id = T1w_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save relevant outputs in a datasink\n",
    "datasink = Node(DataSink(base_directory=exp_dir,\n",
    "                         container=out_dir),\n",
    "                name='datasink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the following naming substitutions for the datasink\n",
    "substitutions = [('_%s' % T1w_id, ''),\n",
    "                 ('_ras', ''),\n",
    "                 ('_ROI', ''),\n",
    "                 ('_corrected', ''),\n",
    "                 ('c1', 'seg_gm_'),\n",
    "                 ('c2', 'seg_wm_'),\n",
    "                 ('c3', 'seg_csf_'),\n",
    "                 ('c4', 'seg_skull_'),\n",
    "                 ('c5', 'seg_head_')\n",
    "                 ]\n",
    "\n",
    "for sub in subject_list:\n",
    "    substitutions += [('sub-%s' % sub, '')]\n",
    "\n",
    "for sess in session_list:\n",
    "    substitutions += [('ses-%s' % sess, '')]\n",
    "\n",
    "for sub in subject_list:\n",
    "    substitutions += [('_subject_id_%s/' % (sub),\n",
    "                       'sub-{0}/sub-{0}_'.format(sub))]\n",
    "    substitutions += [('/sub-%s_.nii' % sub,\n",
    "                       '/sub-%s_T1w_corrected.nii' % sub)]\n",
    "    substitutions += [('/sub-%s__.nii' % sub,\n",
    "                       '/sub-%s_T1w_corrected.nii' % sub)]\n",
    "    for sess in session_list:\n",
    "        substitutions += [('_session_id_{1}sub-{0}/sub-{0}'.format(sub, sess),\n",
    "                           'sub-{0}/sub-{0}_ses-{1}'.format(sub, sess))]\n",
    "        substitutions += [('/sub-%s_ses-%s__.nii' % (sub, sess),\n",
    "                           '/sub-%s_ses-%s_T1w_corrected.nii' % (sub, sess))]\n",
    "\n",
    "substitutions += [('___', '_'),\n",
    "                  ('__', '_'),\n",
    "                  ('_.nii', '.nii')\n",
    "                 ]\n",
    "\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Anatomical Preprocessing Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create anatomical preprocessing workflow\n",
    "preproc_anat = Workflow(name='preproc_anat')\n",
    "preproc_anat.base_dir = work_dir\n",
    "\n",
    "preproc_anat.connect([(info_source, select_files, [('subject_id', 'subject_id'),\n",
    "                                                   ('session_id', 'session_id')]),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add input and output nodes and connect them to the main workflow\n",
    "preproc_anat.connect([(select_files, mainflow, [('anat', 'reorient.in_file')]),\n",
    "                      \n",
    "                      (mainflow, datasink, [\n",
    "                          ('n4.output_image', 'preproc_anat.@n4'),\n",
    "                          ('compressor.compressed_segments', 'preproc_anat.@segment'),\n",
    "                          ('extract_brain.out_file', 'preproc_anat.@brain'),\n",
    "                          ('extract_brain.mask', 'preproc_anat.@mask'),\n",
    "                          ('antsreg.warped_image', 'preproc_anat.@warped_image'),\n",
    "                          ('antsreg.inverse_warped_image', 'preproc_anat.@inverse_warped_image'),\n",
    "                          ('antsreg.composite_transform', 'preproc_anat.@transform'),\n",
    "                          ('antsreg.inverse_composite_transform', 'preproc_anat.@inverse_transform')]),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add input and output nodes and connect them to the report workflow\n",
    "preproc_anat.connect([(info_source, reportflow, [('subject_id', 'create_figures.sub'),\n",
    "                                                 ('session_id', 'create_figures.sess')\n",
    "                                                ]),\n",
    "                      \n",
    "                      (reportflow, datasink, [\n",
    "                          ('create_figures.out_segmentation', 'preproc_anat.@vis_segmentation'),\n",
    "                          ('create_figures.out_brain', 'preproc_anat.@vis_brain'),\n",
    "                          ('create_figures.out_warp', 'preproc_anat.@vis_warp'),\n",
    "                      ]),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect main workflow with report workflow\n",
    "preproc_anat.connect([(mainflow, reportflow, [\n",
    "                        ('n4.output_image', 'create_figures.n4'),\n",
    "                        ('segment.native_class_images', 'create_figures.segments'),\n",
    "                        ('extract_brain.out_file', 'create_figures.brain'),\n",
    "                        ('antsreg.warped_image', 'create_figures.warped_file')\n",
    "                        ]),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preproc_anat output graph\n",
    "preproc_anat.write_graph(graph2use='colored', format='png', simple_form=True)\n",
    "\n",
    "# Visualize the graph in the notebook (NBVAL_SKIP)\n",
    "from IPython.display import Image\n",
    "Image(filename=opj(preproc_anat.base_dir, 'preproc_anat', 'graph.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the workflow in sequential mode\n",
    "res = preproc_anat.run('Linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save workflow graph visualizations in datasink\n",
    "preproc_anat.write_graph(graph2use='flat', format='png', simple_form=True)\n",
    "preproc_anat.write_graph(graph2use='colored', format='png', simple_form=True)\n",
    "\n",
    "from shutil import copyfile\n",
    "copyfile(opj(preproc_anat.base_dir, 'preproc_anat', 'graph.png'),\n",
    "         opj(exp_dir, out_dir, 'preproc_anat', 'graph.png'))\n",
    "copyfile(opj(preproc_anat.base_dir, 'preproc_anat', 'graph_detailed.png'),\n",
    "         opj(exp_dir, out_dir, 'preproc_anat', 'graph_detailed.png'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save template brain in `preproc_anat` folder\n",
    "import shutil\n",
    "from os.path import basename\n",
    "new_path = '/data/derivatives/fmriflows/preproc_anat/%s' % basename(\n",
    "    norm_template)\n",
    "\n",
    "shutil.move(norm_template, new_path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
