{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <p style=\"float: right;\"><img width=\"66%\" src=\"templates/logo_fmriflows.gif\"></p>\n",
    "    <h1>Multivariate Analysis</h1>\n",
    "    <p>This notebook performes a simple multivariate analysis by executing the following steps:\n",
    "\n",
    "1. Collect files and labels\n",
    "1. Create mask to restrict searchlight analysis\n",
    "1. Create PyMVPA dataset and z-score it\n",
    "1. Select target samples\n",
    "1. Run searchlight analysis\n",
    "1. Create outputs 1st-level\n",
    "1. Perform 2nd-level analysis (classical GLM and/or multivariate according to [Stelzer et al. (2013)](https://www.sciencedirect.com/science/article/pii/S1053811912009810))\n",
    "\n",
    "**Note:** This notebook requires that the 1st-level analysis pipeline was already executed, with the parameter `con_per_run` set to `True`, and that it's output can be found in the dataset folder under `/dataset/derivatives/fmriflows/analysis_1stLevel/multivariate`. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure Requirements\n",
    "\n",
    "The data structure to run this notebook should be according to the BIDS format:\n",
    "\n",
    "    dataset\n",
    "    ├── fmriflows_spec_multivariate.json\n",
    "    └── derivatives\n",
    "        └── fmriflows\n",
    "            └── analysis_1stLevel\n",
    "                └── multivariate\n",
    "                    └── sub-{sub_id}\n",
    "                        └── task-{task_id}\n",
    "                            └── tFilter-{tFilter_id}_sFilter-{sFilter_id}\n",
    "                                ├── con_[con_id]_norm.nii.gz\n",
    "                                └── labels.csv\n",
    "\n",
    "`fmriflows` will perform a multivariate (searchlight) analysis on each subjects individually. Type of classifier and possible binary classifications need to be specified in the `fmriflows_spec_multivariate.json` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Specifications\n",
    "\n",
    "This notebook will extract the relevant analysis specifications from the `fmriflows_spec_multivariate.json` file in the dataset folder. In the current setup, they are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join as opj\n",
    "\n",
    "spec_file = opj('/data', 'fmriflows_spec_multivariate.json')\n",
    "\n",
    "with open(spec_file) as f:\n",
    "    specs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters for 1st-level analysis workflow\n",
    "subject_list = specs['subject_list']\n",
    "session_list = specs['session_list']\n",
    "filters_spatial = specs['filters_spatial']\n",
    "filters_temporal = specs['filters_temporal']\n",
    "postfix = specs['multivariate_postfix']\n",
    "clf_names = specs['clf_names']\n",
    "sphere_radius = specs['sphere_radius']\n",
    "sphere_steps = specs['sphere_steps']\n",
    "n_chunks = specs['n_chunks']\n",
    "tasks = specs['tasks']\n",
    "n_perm = specs['n_perm']\n",
    "n_bootstrap = specs['n_bootstrap']\n",
    "block_size = specs['block_size']\n",
    "threshold_group = specs['threshold']\n",
    "multicomp_correction = specs['multicomp_correction']\n",
    "fwe_rate = specs['fwe_rate']\n",
    "atlasreader_names= specs['atlasreader_names']\n",
    "atlasreader_prob_thresh = specs['atlasreader_prob_thresh']\n",
    "n_proc = specs['n_parallel_jobs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to change any of those values manually, overwrite them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subject identifiers\n",
    "subject_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of session identifiers\n",
    "session_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of spatial filters (smoothing) that were used during functional preprocessing\n",
    "filters_spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of temporal filters that were used during functional preprocessing\n",
    "filters_temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a particular analysis postfix\n",
    "postfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of classifier to use. Choose one or many of:\n",
    "#   'LinearCSVMC', 'LinearNuSVMC', 'RbfCSVMC', 'RbfNuSVMC', 'SMLR', 'kNN', 'GNB'\n",
    "clf_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searchlight sphere radius (in voxels), i.e. number of additional voxels\n",
    "#    next to the center voxel. E.g sphere_radius = 3 means radius = 3.5*voxelsize\n",
    "sphere_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of step size to define a sphere center, i.e. value of 5 means\n",
    "#    that only every 5th voxel is used to perform a searchlight analysis\n",
    "sphere_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of chunks to use for the N-Fold crossvalidation\n",
    "#    (needs to divide number of labels without reminder)\n",
    "n_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which classifications should be performed? (separated by task)\n",
    "#    - Classification targets are a tuple of two tuples, indicating\n",
    "#    - Target classification to train and target classification to test\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of permutations to indicate group-analysis strategy:\n",
    "#    n_perm <= 1: group analysis is classical 2nd-level GLM analysis\n",
    "#    n_perm > 1: group analysis is multivariate analysis according to Stelzer et al. (2013)\n",
    "n_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of bootstrap samples to be generated\n",
    "n_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of elements per segment used to compute the feature-wise NULL distributions\n",
    "# Low number mean low memory demand and slow computation time\n",
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-wise probability threshold per voxel\n",
    "threshold_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy for multiple comparison correction, options are: 'bonferroni', 'sidak',\n",
    "# 'holm-sidak', 'holm', 'simes-hochberg', 'hommel', 'fdr_bh', 'fdr_by', None\n",
    "multicomp_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Family-wise error rate threshold for multiple comparison correction\n",
    "fwe_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of atlases to use for creation of output tables\n",
    "atlasreader_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability threshold to use for output tables\n",
    "atlasreader_prob_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parallel jobs to run\n",
    "n_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations\n",
    "\n",
    "First things first, let's import important modules and specify relevant environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from os.path import basename\n",
    "from mvpa2.suite import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths and names\n",
    "exp_dir = '/data/derivatives'\n",
    "out_dir = 'fmriflows'\n",
    "work_dir = '/workingdir'\n",
    "\n",
    "# Create multivariate output workflow\n",
    "out_folder_name = 'analysis_2ndLevel'\n",
    "if postfix:\n",
    "    out_folder_name += '_%s' % postfix\n",
    "out_path = opj(exp_dir, out_dir, out_folder_name, 'multivariate')\n",
    "\n",
    "# Create output folder\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definition\n",
    "\n",
    "In this section, we will define all the functions that we need to run the searchlight analysis.\n",
    "\n",
    "## Collect files\n",
    "\n",
    "This function will return a list containing the beta-maps, a list containing the corresponding labels and a list that specifies which labels and contrasts belong to which chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_files(subject_id, session_id, task_id, tFilter_id, sFilter_id, n_chunks, postfix):\n",
    "\n",
    "    \"\"\"This function collects the relevant input files, labels and chunk_idx\"\"\"\n",
    "    \n",
    "    from glob import glob\n",
    "    template_con = '/data/derivatives/fmriflows/analysis_1stLevel'\n",
    "    if postfix:\n",
    "        template_con += '_%s' % postfix\n",
    "    template_con += '/multivariate/sub-{0}/task-{1}/'\n",
    "    if session_id:\n",
    "        template_con += 'ses-%s/' % session_id\n",
    "    template_con += '{2}_{3}/{4}'\n",
    "    \n",
    "    tFilter_id = 'tFilter_%s.%s' % (tFilter[0], tFilter[1])\n",
    "    sFilter_id = 'sFilter_%s.%s' % (sFilter[0], sFilter[1])\n",
    "\n",
    "    # Collect files\n",
    "    from glob import glob\n",
    "    con_files = glob(template_con.format(\n",
    "        subject_id, task_id, tFilter_id, sFilter_id, 'con_*_norm.nii???'))\n",
    "    labels_file = glob(template_con.format(\n",
    "        subject_id, task_id, tFilter_id, sFilter_id, 'labels.csv'))\n",
    "\n",
    "    # Extract content from label file\n",
    "    labels = list(np.ravel([np.loadtxt(l, dtype='S') for l in labels_file]))\n",
    "    \n",
    "    # Create chunks index\n",
    "    chunks = [i for i in range(n_chunks)\n",
    "              for j in range(int(len(con_files) / n_chunks))]\n",
    "    \n",
    "    # Make sure that that you have same number of contrasts and labels\n",
    "    assert(len(con_files)==len(labels)==len(chunks))\n",
    "    \n",
    "    return sorted(con_files), labels, chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify mask\n",
    "\n",
    "This function will create and return a binary mask from the gray matter probability template to restrict the searchlight analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import concat_imgs, math_img\n",
    "from scipy.ndimage.morphology import binary_dilation, binary_erosion, binary_fill_holes\n",
    "from nilearn.image import new_img_like\n",
    "\n",
    "def create_mask(con_files):\n",
    "    \n",
    "    \"\"\"\n",
    "    Restrict the searchlight analysis to a mask containing only voxels\n",
    "    with values, fill holes in this mask and dilate it by one.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find only voxels with values at least in one contrast\n",
    "    img_mask = math_img('np.sum(np.abs(img), axis=-1)!=0', img=concat_imgs(con_files))\n",
    "\n",
    "    # Dilate mask twice, fill holes and erode once\n",
    "    data_mask = binary_dilation(img_mask.get_data(), iterations=2)\n",
    "    data_mask = binary_erosion(binary_fill_holes(data_mask), iterations=1)\n",
    "\n",
    "    # Save in new NIfTI image\n",
    "    img_mask = new_img_like(img_mask, data_mask, copy_header=True)\n",
    "\n",
    "    return img_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searchlight results function\n",
    "\n",
    "This function takes the results aggregated from the searchlight analysis and stores in every voxel of the mask the average value of each sphere that included this particular voxel. This function has therefore an effect of **smoothing the results**. Additionally, this function also allows to **fill up wholes** in the searchlight map if not every voxel was used as a center of a sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_scattered_results(sl, dataset, roi_ids, results):\n",
    "\n",
    "    \"\"\"Function to aggregate results - This requires the searchlight\n",
    "    conditional attribute 'roi_feature_ids' to be enabled\"\"\"\n",
    "    \n",
    "    resmap = None\n",
    "    for resblock in results:\n",
    "        for res in resblock:\n",
    "            if resmap is None:\n",
    "                \n",
    "                # prepare the result container\n",
    "                resmap = np.zeros((len(res), dataset.nfeatures),\n",
    "                                  dtype=res.samples.dtype)\n",
    "                observ_counter = np.zeros(dataset.nfeatures, dtype=int)\n",
    "            \n",
    "            # project the result onto all features -- love broadcasting!\n",
    "            resmap[:, res.a.roi_feature_ids] += res.samples\n",
    "            \n",
    "            # increment observation counter for all relevant features\n",
    "            observ_counter[res.a.roi_feature_ids] += 1\n",
    "    \n",
    "    # when all results have been added up average them according to the number\n",
    "    # of observations\n",
    "    observ_mask = observ_counter > 0\n",
    "    resmap[:, observ_mask] /= observ_counter[observ_mask]\n",
    "    result_ds = Dataset(resmap,\n",
    "                        fa={'observations': observ_counter})\n",
    "    \n",
    "    if 'mapper' in dataset.a:\n",
    "        import copy\n",
    "        result_ds.a['mapper'] = copy.copy(dataset.a.mapper)\n",
    "    \n",
    "    return result_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier selection\n",
    "\n",
    "This function return the classifier object defined by the classifier name `clf_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvpa2.clfs.svm import LinearCSVMC, LinearNuSVMC, RbfCSVMC, RbfNuSVMC\n",
    "from mvpa2.clfs.smlr import SMLR\n",
    "from mvpa2.clfs.knn import kNN\n",
    "from mvpa2.clfs.gnb import GNB\n",
    "\n",
    "def get_classifier(clf_name):\n",
    "    \n",
    "    \"\"\"Returns specified classifier object\"\"\"\n",
    "    clfs = {\n",
    "        'LinearCSVMC': LinearCSVMC(),\n",
    "        'LinearNuSVMC': LinearNuSVMC(),\n",
    "        'RbfCSVMC': RbfCSVMC(),\n",
    "        'RbfNuSVMC': RbfNuSVMC(),\n",
    "        'SMLR': SMLR(),\n",
    "        'kNN': kNN(k=3),\n",
    "        'GNB': GNB(),\n",
    "    }\n",
    "\n",
    "    return clfs[clf_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMVPA dataset creation\n",
    "\n",
    "This function creates the dataset object `ds` needed for the searchlight analysis. This is also where the data is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvpa2.base.hdf5 import h5save\n",
    "from mvpa2.datasets.mri import fmri_dataset\n",
    "from mvpa2.mappers.zscore import zscore\n",
    "\n",
    "def create_dataset(subject_id, session_id, task_id, tFilter, sFilter, n_chunks, out_path, postfix):\n",
    "    \n",
    "    \"\"\"Create PyMVPA dataset for given input parameters and stores it in a HDF5 file\"\"\"\n",
    "    \n",
    "    # Create filter idx\n",
    "    tFilter_id = '%s.%s' % (tFilter[0], tFilter[1])\n",
    "    sFilter_id = '%s.%s' % (sFilter[0], sFilter[1])\n",
    "\n",
    "    # Collect files, labels and chunks\n",
    "    con_files, labels, chunks = collect_files(subject_id, session_id, task_id,\n",
    "                                              tFilter_id, sFilter_id, n_chunks, postfix)\n",
    "    \n",
    "    # Create binary mask from con files\n",
    "    mask_img = create_mask(con_files)\n",
    "    \n",
    "    # Create dataset\n",
    "    ds = fmri_dataset(samples=con_files,\n",
    "                      targets=labels,\n",
    "                      chunks=chunks,\n",
    "                      mask=mask_img)\n",
    "    del ds.sa['time_coords']\n",
    "    del ds.sa['time_indices']\n",
    "\n",
    "    # Normalize dataset\n",
    "    zscore(ds)\n",
    "    \n",
    "    # Save dataset in HDF5 format and return in\n",
    "    ds_name = 'sub-%s_tFilter-%s_sFilter-%s.hdf5' % (\n",
    "        subject_id, tFilter_id, sFilter_id)\n",
    "    if session_id:\n",
    "        ds_name.replace('_tFilter', 'ses-%s_tFilter' % session_id)\n",
    "    ds_path = opj(out_path, 'task-%s' % task_id, ds_name)\n",
    "    h5save(ds_path, ds)\n",
    "    \n",
    "    return ds_path, ds_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create searchlight specific dataset\n",
    "\n",
    "This function takes the PyMVPA dataset and prepares it for the searchlight analysis. In particular, it removes the target labels that are not needed for the classification and prepares the dataset for the cross-classification (training on one group to predict another) or the standard classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(ds_path, train_labels, test_labels, cross_clf):\n",
    "\n",
    "    \"\"\"Prepares the dataset for the searchlight classification\"\"\"\n",
    "\n",
    "    # Load dataset and extract labels and chunks\n",
    "    ds = h5load(ds_path)\n",
    "    labels = np.copy(ds.targets)\n",
    "    chunks = np.copy(ds.chunks)\n",
    "    \n",
    "    # Select targets and rename chunks and labels if necessary\n",
    "    if cross_clf:\n",
    "        selecter = np.isin(labels, train_labels + test_labels)\n",
    "        chunks = [int((l in test_labels) and ((l not in train_labels))) for l in labels[selecter]]\n",
    "        selection = labels[selecter]\n",
    "        for i, l in enumerate(test_labels):\n",
    "            selection[np.argwhere(selection==test_labels[i])] = train_labels[i]\n",
    "    else:\n",
    "        selecter = np.isin(labels, train_labels)\n",
    "        chunks = chunks[selecter]\n",
    "        selection = labels[selecter]\n",
    "\n",
    "    # Create searchlight analysis specifc dataset\n",
    "    ds_sl = ds[selecter]\n",
    "    ds_sl.sa.chunks = chunks\n",
    "    ds_sl.sa.targets = selection\n",
    "\n",
    "    return ds_sl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run searchlight analysis\n",
    "\n",
    "This function runs the searchlight analysis with the given input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvpa2.generators.partition import NFoldPartitioner\n",
    "from mvpa2.measures.base import CrossValidation\n",
    "from mvpa2.measures.searchlight import sphere_searchlight\n",
    "from mvpa2.misc.errorfx import mean_match_accuracy\n",
    "from mvpa2.mappers.fx import mean_sample\n",
    "from mvpa2.base import debug\n",
    "\n",
    "def run_searchlight(ds_sl, clf_name, cross_clf, sphere_radius, sphere_steps, n_proc, verbose=False):\n",
    "\n",
    "    \"\"\"Run Searchlight Analysis and return searchlight output\"\"\"\n",
    "    \n",
    "    # Specify cross-validation scheme\n",
    "    partitioner = NFoldPartitioner(cvtype=1)\n",
    "    \n",
    "    # Specify classifier\n",
    "    clf = get_classifier(clf_name)\n",
    "    \n",
    "    # Create cross validation object\n",
    "    if cross_clf:\n",
    "        cv = CrossValidation(clf,\n",
    "                             partitioner,\n",
    "                             errorfx=mean_match_accuracy,\n",
    "                             enable_ca=['stats'],\n",
    "                             splitter=Splitter(attr='chunks',\n",
    "                                               attr_values=(0, 1)))\n",
    "    else:\n",
    "        cv = CrossValidation(clf,\n",
    "                             partitioner,\n",
    "                             errorfx=mean_match_accuracy,\n",
    "                             enable_ca=['stats'])\n",
    "\n",
    "    # Create searchlight object\n",
    "    sl = sphere_searchlight(cv,\n",
    "                            radius=sphere_radius,\n",
    "                            center_ids=range(0,\n",
    "                                             ds_sl.shape[1],\n",
    "                                             sphere_steps),\n",
    "                            space='voxel_indices',\n",
    "                            results_fx=fill_in_scattered_results,\n",
    "                            postproc=mean_sample(),\n",
    "                            enable_ca=['calling_time', 'roi_feature_ids'],\n",
    "                            nproc=n_proc)\n",
    "    \n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    # Turn verbose on or off\n",
    "    if verbose:\n",
    "        debug.active = [\"SLC\"]\n",
    "    else:\n",
    "        debug.active = []\n",
    "    \n",
    "    # Run searchlight analysis\n",
    "    sl_map = sl(ds_sl)\n",
    "    \n",
    "    return sl, sl_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create informative logfile\n",
    "\n",
    "This function writes relevant searchlight analysis information to a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvpa2.suite import time\n",
    "\n",
    "def create_output(ds, sl, sl_map, subject_id, clf_name, targets, sphere_radius,\n",
    "                  sphere_steps, ds_name, clf_type, result_path, n_proc):\n",
    "    \n",
    "    \"\"\"Save important model information in a text file.\"\"\"\n",
    "\n",
    "    # Extract important information\n",
    "    wall_time = time.strftime('%H:%M:%S', time.gmtime(round(sl.ca.calling_time)))\n",
    "\n",
    "    # Accuracy information\n",
    "    accuracies = sl_map.S[0]\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    chance_level = 1.0 / len(np.unique(ds.sa.targets))\n",
    "\n",
    "    # Helper functions\n",
    "    def threshold_above_average(x):\n",
    "        return chance_level + x * std_accuracy\n",
    "\n",
    "    def spheres_above_average(x):\n",
    "        return np.sum(accuracies >= threshold_above_average(x))\n",
    "\n",
    "    def percent_above_average(x):\n",
    "        return np.mean(accuracies >= threshold_above_average(x)) * 100\n",
    "    \n",
    "    # Output text\n",
    "    txt = ['Subject          : {0}'.format(subject_id),\n",
    "           'Classifier       : {0}'.format(clf_name),\n",
    "           'Classes          : {0}'.format(targets),\n",
    "           'Targets          : {0}'.format(ds.targets.tolist()),\n",
    "           'Chunks           : {0}'.format(ds.chunks.tolist()),\n",
    "           'Sphere Radius    : {0}'.format(sphere_radius),\n",
    "           'N-th Element     : {0}'.format(sphere_steps),\n",
    "           'Wall Time        : {0}'.format(wall_time),\n",
    "           'Samples          : {0}'.format(ds.S.shape[0]),\n",
    "           'Features         : {0}'.format(ds.S.shape[1]),\n",
    "           'Volume Dimension : {0}'.format(str(ds.a.voxel_dim)),\n",
    "           'Voxel  Dimension : {0}'.format(str(ds.a.voxel_eldim)),\n",
    "           'CPU              : {0}'.format(n_proc),\n",
    "           \n",
    "           '\\nChance Level     : {0:>5}'.format(round(chance_level, 5)*100),\n",
    "           'Accuracy (mean)  : {0:>5}'.format(round(mean_accuracy, 5)*100),\n",
    "           'Accuracy (std)   : {0:>5}'.format(round(std_accuracy, 5)*100),\n",
    "           \n",
    "           'above 2STD in %  : {0:>7}%'.format(round(percent_above_average(2), 3)),\n",
    "           'above 3STD in %  : {0:>7}%'.format(round(percent_above_average(3), 3)),\n",
    "           'above 2STD in v  : {0:>7}'.format(spheres_above_average(2)),\n",
    "           'above 3STD in v  : {0:>7}'.format(spheres_above_average(3)),\n",
    "           \n",
    "           '\\nDataset Summary:',\n",
    "           '****************',\n",
    "           '%s' % ds.summary(),\n",
    "           '%s' % ds.sa,\n",
    "           '\\n%s' % ds.summary]\n",
    "\n",
    "    # Write information to text file\n",
    "    results_file = opj(result_path, ds_name.replace('.hdf5', '_%s.rst' % clf_type))\n",
    "    with open(results_file, 'w') as f:\n",
    "        f.writelines('\\n'.join(txt))\n",
    "        \n",
    "    # Return relevant information\n",
    "    results = [wall_time, chance_level, mean_accuracy, std_accuracy,\n",
    "               spheres_above_average(2), spheres_above_average(3)]\n",
    "    return results_file, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save searchlight result in NIfTI and show it in glassbrain plot\n",
    "\n",
    "This function stores the searchlight results in a NIfTI file and shows the output on a glassbrain plot, thresholded at a given value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mvpa2.datasets.mri import map2nifti\n",
    "from nilearn.plotting import plot_glass_brain\n",
    "from nilearn.image import new_img_like\n",
    "\n",
    "def save_nifti_as_outputs(ds, sl_map, results_file, threshold):\n",
    "\n",
    "    # Put searchlight output back into NIfTI space\n",
    "    sl_data = ds.mapper.reverse(sl_map.S)[0, ...]\n",
    "    sl_img = new_img_like(map2nifti(ds), sl_data, affine=ds.a.imgaffine)\n",
    "    sl_filename = results_file.replace('.rst', '.nii.gz')\n",
    "    sl_img.to_filename(opj(result_path, sl_filename))\n",
    "\n",
    "    # Plotting the searchlight results on the glass brain\n",
    "    title_txt = '{} - threshold = {}%'.format(clf_type, round(threshold * 100, 2))\n",
    "    plot_glass_brain(sl_img, black_bg=True, colorbar=True, title=title_txt,\n",
    "                     display_mode='lyrz', threshold=threshold, cmap='magma',\n",
    "                     output_file=results_file.replace('.rst', '.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Searchlight Analysis\n",
    "\n",
    "Now that all important functions are specified, we can perform the searchlight analysis. As with a classical design, searchlight analysis apply a 1st-level and 2nd-level analysis. The **correct approach** to perform a 2nd-level analysis on searchlight results, i.e. accuracy maps, involves permutation (see [Stelzer et al. (2013)](https://www.sciencedirect.com/science/article/pii/S1053811912009810)). This notebook nonetheless, applies also a classical 2nd-level GLM, that can be used as an indication of results, but shouldn't be used for publications!\n",
    "\n",
    "In the rest of the notebook the following steps are conducted:\n",
    "\n",
    "1. **1st-level searchlight analysis** (with original labels) to acquire accuracy maps\n",
    "1. **classical 2nd-level GLM analysis**, based on those accuracy maps\n",
    "1. **1st-level searchlight analysis** with **permutated labels** (performed N-times)\n",
    "1. **2nd-level analysis** according **[Stelzer et al. (2013)](https://www.sciencedirect.com/science/article/pii/S1053811912009810)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st-level searchlight analysis (with original labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify order of parameter to iterate over\n",
    "if not session_list:\n",
    "    session_list = ['']\n",
    "\n",
    "iteration_list = [[s, t, tf, sf, sess]\n",
    "                  for sess in session_list\n",
    "                  for tf in filters_temporal\n",
    "                  for sf in filters_spatial\n",
    "                  for t in tasks.keys()\n",
    "                  for s in subject_list]\n",
    "iteration_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over all parameters and run the searchlight analysis\n",
    "for subject_id, task_id, tFilter, sFilter, session_id in iteration_list:\n",
    "\n",
    "    # Create multivariate dataset\n",
    "    ds_path, ds_name = create_dataset(\n",
    "        subject_id, session_id, task_id, tFilter, sFilter,\n",
    "        n_chunks, out_path, postfix)\n",
    "\n",
    "    # Go through all targets\n",
    "    for targets in tasks[task_id]:\n",
    "\n",
    "        # Extract training and testing labels\n",
    "        train_labels = targets[0]\n",
    "        test_labels = targets[1]\n",
    "\n",
    "        # Specify results folder\n",
    "        result_path = opj(out_path, 'task-%s' % task_id, 'train-%s_test-%s' % (\n",
    "            '_'.join([str(s) for s in targets[0]]),\n",
    "            '_'.join([str(s) for s in targets[1]])))\n",
    "        if not os.path.exists(result_path):\n",
    "            os.makedirs(result_path)\n",
    "\n",
    "        # Are the training and target labels the same?\n",
    "        cross_clf = targets[0] != targets[1]\n",
    "\n",
    "        # Prepare PyMVPA dataset for searchlight analysis\n",
    "        ds_sl = prepare_dataset(ds_path, train_labels, test_labels, cross_clf)\n",
    "\n",
    "        # Go through specified classifiers\n",
    "        for clf_name in clf_names:\n",
    "\n",
    "            # Perform searchlight analysis\n",
    "            sl, sl_map = run_searchlight(\n",
    "                ds_sl, clf_name, cross_clf, sphere_radius, sphere_steps, n_proc,\n",
    "                verbose=True)\n",
    "\n",
    "            # Create visual outputs and report file\n",
    "            clf_type = 'clf-%s_radius-%0d_steps-%03d' % (\n",
    "                clf_name, sphere_radius, sphere_steps)\n",
    "            results_file, results = create_output(\n",
    "                ds_sl, sl, sl_map, subject_id, clf_name, targets,\n",
    "                sphere_radius, sphere_steps, ds_name, clf_type,\n",
    "                result_path, n_proc)\n",
    "            wall_time, chance_level, mean_accuracy, std_accuracy, v2STD, v3STD = results\n",
    "            threshold = mean_accuracy + 2 * std_accuracy\n",
    "            save_nifti_as_outputs(ds_sl, sl_map, results_file, threshold)\n",
    "\n",
    "            # Print log stream to terminal\n",
    "            out_stream = '{} - Targets: {}'.format(\n",
    "                ds_name.replace('.hdf5', ''), '_'.join([str(t) for t in targets]))\n",
    "            out_stream += '\\n  {}  Radius: {:>5}   Steps: {:>4}   CLF: {}'.format(\n",
    "                wall_time, sphere_radius, sphere_steps, clf_name)\n",
    "            out_stream += '\\n            Chance: {:>5}%  Mean: {:>5}%  STD: {}%  +2STDv={} +3STDv={}'.format(\n",
    "                round(100*chance_level, 1), round(100*mean_accuracy, 1), round(100*std_accuracy, 1), v2STD, v3STD)\n",
    "            border_txt = '#' * 40\n",
    "            print('\\n'.join([border_txt, out_stream, border_txt]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical 2nd-level GLM analysis\n",
    "\n",
    "The application of a classical GLM approach to perform a 2nd-level analysis of searchlight results is not recommended. The correct way to perform a group analysis on searchlight accuracy maps is by applying permutation testing, as for example proposed by [Stelzer et al. (2013)](https://www.sciencedirect.com/science/article/pii/S1053811912009810)).\n",
    "\n",
    "Having said all this, the following code performs a classical 2nd-level GLM analysis and tests the acquired accuracy maps against chance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify order of parameter to iterate over\n",
    "if not session_list:\n",
    "    session_list = ['']\n",
    "    \n",
    "iteration_list = [[t, tf, sf, sess]\n",
    "                  for sess in session_list\n",
    "                  for tf in filters_temporal\n",
    "                  for sf in filters_spatial\n",
    "                  for t in tasks.keys()]\n",
    "iteration_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from nilearn.image import concat_imgs, math_img\n",
    "from nilearn.plotting import plot_glass_brain\n",
    "from nistats.second_level_model import SecondLevelModel\n",
    "from nistats.thresholding import map_threshold\n",
    "\n",
    "# Iterate over all parameters and run the searchlight analysis\n",
    "for task_id, tFilter, sFilter, session_id in iteration_list:\n",
    "\n",
    "    # Create filter idx\n",
    "    tFilter_id = '%s.%s' % (tFilter[0], tFilter[1])\n",
    "    sFilter_id = '%s.%s' % (sFilter[0], sFilter[1])\n",
    "\n",
    "    # Go through all targets\n",
    "    for targets in tasks[task_id]:\n",
    "        \n",
    "        # Specify classification folder\n",
    "        clf_folder = 'train-%s_test-%s' % (\n",
    "            '_'.join([str(s) for s in targets[0]]),\n",
    "            '_'.join([str(s) for s in targets[1]]))\n",
    "\n",
    "        # Go through specified classifiers\n",
    "        for clf_name in clf_names:\n",
    "            \n",
    "            # Collect all the files\n",
    "            file_idx = 'sub-*_tFilter-%s_sFilter-%s' % (tFilter_id, sFilter_id)\n",
    "            file_idx += '_clf-%s_radius-%0d_steps-%03d.nii.gz' % (clf_name, sphere_radius, sphere_steps)\n",
    "            sl_res = sorted(glob(opj(out_path, 'task-%s' % task_id, clf_folder, file_idx)))\n",
    "            \n",
    "            # Compute group mask\n",
    "            group_mask = math_img('np.sum(img!=0, axis=-1)==img.shape[3]',\n",
    "                                  img=concat_imgs(sl_res))\n",
    "            \n",
    "            # Path to output folder\n",
    "            result_path = opj(out_path, 'task-%s' % task_id, 'group_glm', clf_folder)\n",
    "            if not os.path.exists(result_path):\n",
    "                os.makedirs(result_path)\n",
    "                \n",
    "            # Center searchlight accuracy maps around chance level\n",
    "            chance_level = 1.0 / len(targets[0])\n",
    "            sl_imgs = [math_img('(img - %s) * mask' % chance_level,\n",
    "                                img=sl, mask=group_mask) for sl in sl_res]\n",
    "            \n",
    "            # Specify 2nd-level GLM design matrix\n",
    "            design_matrix = pd.DataFrame([1] * len(sl_res), columns=['intercept'])\n",
    "\n",
    "            # Estimate 2nd-level Model\n",
    "            second_level_model = SecondLevelModel()\n",
    "            second_level_model = second_level_model.fit(\n",
    "                sl_imgs, design_matrix=design_matrix)\n",
    "\n",
    "            # Compute z-score\n",
    "            z_map = second_level_model.compute_contrast(output_type='z_score')\n",
    "            out_filename = opj(result_path, file_idx.replace('sub-*', 'z-map'))\n",
    "            z_map.to_filename(out_filename)\n",
    "\n",
    "            # Threshold output with different approaches and save figure\n",
    "            for l, h in [(.001, 'fpr'), (.05, 'fdr'), (.05, 'bonferroni')]:\n",
    "            \n",
    "                thr_name = out_filename.replace('.nii.gz', 'thr_%s_%.03f.nii.gz' % (h, l))\n",
    "                thr_map, thr = map_threshold(z_map, level=l, height_control=h)\n",
    "                thr_map.to_filename(thr_name)\n",
    "                \n",
    "                plot_glass_brain(\n",
    "                    z_map, threshold=thr, colorbar=True, black_bg=True, plot_abs=False,\n",
    "                    display_mode='lyrz',  output_file=thr_name.replace('.nii.gz', '.png'),\n",
    "                    title='%s: %s - Threshold = %.03f' % (h, l, thr))\n",
    "            \n",
    "            log_txt = file_idx.replace('sub-*_', '').replace('.nii.gz', '').split('_')\n",
    "            print('{:<20} {:<16} {:<20} {:<10} {:<10} finished.'.format(*log_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that permutations were requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if `n_perm <= 1`. If this is true, stop the execution of this notebook here:\n",
    "assert (n_perm <= 1) == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st-level searchlight analysis (with label permutation)\n",
    "\n",
    "Depending on the number of permutations per subject, this step will take a long time to compute. It is recommended to take the following code and run it on a cluster server, where you can profit from real parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify order of parameter to iterate over\n",
    "if not session_list:\n",
    "    session_list = ['']\n",
    "\n",
    "iteration_list = [[s, t, tf, sf, sess]\n",
    "                  for sess in session_list\n",
    "                  for tf in filters_temporal\n",
    "                  for sf in filters_spatial\n",
    "                  for t in tasks.keys()\n",
    "                  for s in subject_list]\n",
    "iteration_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over all parameters and run the searchlight analysis\n",
    "for subject_id, task_id, tFilter, sFilter, session_id in iteration_list:\n",
    "\n",
    "    # Create multivariate dataset\n",
    "    ds_path, ds_name = create_dataset(\n",
    "        subject_id, session_id, task_id, tFilter, sFilter,\n",
    "        n_chunks, out_path, postfix)\n",
    "\n",
    "    # Create filter idx\n",
    "    tFilter_id = '%s.%s' % (tFilter[0], tFilter[1])\n",
    "    sFilter_id = '%s.%s' % (sFilter[0], sFilter[1])\n",
    "\n",
    "    # Go through all targets\n",
    "    for targets in tasks[task_id]:\n",
    "\n",
    "        # Extract training and testing labels\n",
    "        train_labels = targets[0]\n",
    "        test_labels = targets[1]\n",
    "\n",
    "        # Specify results folder\n",
    "        result_path = opj(out_path, 'task-%s' % task_id, 'train-%s_test-%s' % (\n",
    "            '_'.join([str(s) for s in targets[0]]),\n",
    "            '_'.join([str(s) for s in targets[1]])), 'permutations')\n",
    "        if not os.path.exists(result_path):\n",
    "            os.makedirs(result_path)\n",
    "\n",
    "        # Are the training and target labels the same?\n",
    "        cross_clf = targets[0] != targets[1]\n",
    "\n",
    "        # Prepare PyMVPA dataset for searchlight analysis\n",
    "        ds_sl = prepare_dataset(ds_path, train_labels, test_labels, cross_clf)\n",
    "\n",
    "        # Go through specified classifiers\n",
    "        for clf_name in clf_names:\n",
    "\n",
    "            # Go through all permutations, but verify that none occures double\n",
    "            perm = AttributePermutator('targets', limit='chunks')\n",
    "\n",
    "            # Add original combination strings to list of used combinations\n",
    "            used_combinations = []\n",
    "            orig_combination = ds_sl.targets!=ds_sl.targets[0]\n",
    "            new_comb_txt1 = ''.join(['1' if c else '0' for c in orig_combination])\n",
    "            new_comb_txt2 = ''.join(['0' if c else '1' for c in orig_combination])\n",
    "            used_combinations += [new_comb_txt1, new_comb_txt2]\n",
    "\n",
    "            perm_step = 0\n",
    "            while perm_step < n_perm - 1:\n",
    "\n",
    "                # Permutate dataset\n",
    "                ds_perm = perm(ds_sl)\n",
    "                new_comb = ds_perm.targets!=ds_sl.targets[0]\n",
    "                new_comb_txt1 = ''.join(['1' if c else '0' for c in new_comb])\n",
    "                new_comb_txt2 = ''.join(['0' if c else '1' for c in new_comb])\n",
    "\n",
    "                # Catch searchlight analysis that break\n",
    "                try:\n",
    "                \n",
    "                    # Verify that permutation is unique\n",
    "                    counter = 0\n",
    "                    new_comb_found = True\n",
    "\n",
    "                    while new_comb_txt1 in used_combinations and new_comb_found:\n",
    "                        ds_perm = perm(ds_sl)\n",
    "                        new_comb = ds_perm.targets!=ds_sl.targets[0]\n",
    "                        new_comb_txt1 = ''.join(['1' if c else '0' for c in new_comb])\n",
    "                        new_comb_txt2 = ''.join(['0' if c else '1' for c in new_comb])\n",
    "                        counter += 1\n",
    "                        if counter >= n_perm:\n",
    "                            new_comb_found = False\n",
    "\n",
    "                    # Perform searchlight if permutation limit is not reached yet\n",
    "                    if new_comb_found:\n",
    "\n",
    "                        used_combinations += [new_comb_txt1, new_comb_txt2]\n",
    "\n",
    "                        sl, sl_map = run_searchlight(\n",
    "                            ds_perm, clf_name, cross_clf, sphere_radius, sphere_steps, n_proc,\n",
    "                            verbose=False)\n",
    "\n",
    "                        # Create report file\n",
    "                        clf_type = 'clf-%s_radius-%0d_steps-%03d_perm-%s' % (\n",
    "                            clf_name, sphere_radius, sphere_steps, new_comb_txt1)\n",
    "                        results_file, results = create_output(\n",
    "                            ds_perm, sl, sl_map, subject_id, clf_name, targets,\n",
    "                            sphere_radius, sphere_steps, ds_name, clf_type,\n",
    "                            result_path, n_proc)\n",
    "                        \n",
    "                        # Save result in a numpy object\n",
    "                        out_name = 'sub-%s_tFilter-%s_sFilter-%s_%s' % (\n",
    "                            subject_id, tFilter_id, sFilter_id, clf_type)\n",
    "\n",
    "                        if session_id:\n",
    "                            out_name.replace('_tFilter', 'ses-%s_tFilter' % session_id)\n",
    "\n",
    "                        # Put permutation output back into NIfTI space\n",
    "                        sl_data = ds_sl.mapper.reverse(sl_map.S)[0, ...]\n",
    "                        sl_img = new_img_like(map2nifti(ds_sl), sl_data,\n",
    "                                              affine=ds_sl.a.imgaffine)\n",
    "                        sl_filename = results_file.replace('.rst', '.nii.gz')\n",
    "                        sl_img.to_filename(sl_filename)\n",
    "\n",
    "                        # Print log stream to terminal\n",
    "                        wall_time = time.strftime('%H:%M:%S', time.gmtime(round(sl.ca.calling_time)))\n",
    "                        print('{} {}'.format(wall_time, basename(out_name)))\n",
    "\n",
    "                    perm_step += 1\n",
    "                \n",
    "                except FailedToTrainError as e:\n",
    "                    print 'Searchlight was restarted because of error: %s' % e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd-level analysis according to [Stelzer et al. (2013)](https://www.sciencedirect.com/science/article/pii/S1053811912009810)\n",
    "\n",
    "This next step is also time consuming, but luckily is much lower than the previous permutation process. It should be finished within a few hours.\n",
    "\n",
    "But first we need to define two support functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvpa2.algorithms.group_clusterthr import GroupClusterThreshold\n",
    "from mvpa2.base import debug\n",
    "\n",
    "def run_stelzer(ds_orig, ds_perm, n_bootstrap, block_size, threshold,\n",
    "                multicomp_correction, fwe_rate, verbose=False):\n",
    "\n",
    "    \"\"\"Performs group cluster thresholding according to Stelzer et al., 2013\"\"\"\n",
    "    \n",
    "    # Turn verbose on or off\n",
    "    if verbose:\n",
    "        debug.active = [\"GCTHR\"]\n",
    "    else:\n",
    "        debug.active = []\n",
    "\n",
    "    # Compute group clustering threshold\n",
    "    n_blocks = int(float(ds_orig.shape[1]) / block_size)\n",
    "    thr = GroupClusterThreshold(n_bootstrap=n_bootstrap,\n",
    "                                feature_thresh_prob=threshold,\n",
    "                                chunk_attr='subj',\n",
    "                                fwe_rate=fwe_rate,\n",
    "                                multicomp_correction=multicomp_correction,\n",
    "                                n_blocks=n_blocks,\n",
    "                                n_proc=n_proc)\n",
    "    thr.train(ds_perm)\n",
    "    sl_res = thr(ds_orig)\n",
    "    \n",
    "    return sl_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "\n",
    "def run_atlasreader(file_id, atlasreader_names, atlasreader_prob_thresh):\n",
    "    \n",
    "    \"\"\"Run atlasreader under conda environment 'neuro' to create outputs\"\"\"\n",
    "    \n",
    "    # Get lowest non-zero value in image\n",
    "    from nibabel import load\n",
    "    data = load(file_id).get_data()\n",
    "    results = data[data!=0]\n",
    "    if len(results):\n",
    "        threshold = data[data!=0].min()\n",
    "    else:\n",
    "        threshold = 0\n",
    "    \n",
    "    cmd = ['/opt/miniconda-latest/envs/neuro/bin/atlasreader',\n",
    "           '-a', atlasreader_names,\n",
    "           '-t', str(threshold),\n",
    "           '-p', str(atlasreader_prob_thresh),\n",
    "           file_id,\n",
    "           '0']\n",
    "    \n",
    "    call(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nb\n",
    "from mvpa2.datasets.mri import map2nifti\n",
    "import subprocess\n",
    "\n",
    "def store_stelzer_results(\n",
    "    sl_res, n_bootstrap, threshold_group, multicomp_correction, fwe_rate,\n",
    "    file_idx, result_path, atlasreader_names, atlasreader_prob_thresh):\n",
    "    \n",
    "    \"\"\"Function to store results from thresholded group analysis\"\"\"\n",
    "    \n",
    "    # File identifier for output creation\n",
    "    identifier = 'b-%06d_thr-%.04f_corr-%s_FWE-%.03f' % (\n",
    "        n_bootstrap, threshold_group, multicomp_correction, fwe_rate)\n",
    "    identifier += file_idx[5:-7]\n",
    "\n",
    "    # Store results in hdf5 file\n",
    "    h5save(opj(result_path, 'grp_stats_%s.hdf5' % identifier), sl_res, compression=9)\n",
    "\n",
    "    # Store average accuracy of group\n",
    "    nb.save(map2nifti(sl_res, sl_res.samples),\n",
    "            opj(result_path, 'grp_01_avg_acc_%s.nii.gz' % identifier))\n",
    "\n",
    "    # Store feature-wise cluster-forming thresholds\n",
    "    nb.save(map2nifti(sl_res, sl_res.fa.featurewise_thresh),\n",
    "            opj(result_path, 'grp_02_featurewise_thresh_%s.nii.gz' % identifier))\n",
    "    \n",
    "    # Store cluster labels after thresholding - largest cluster starts with `1`\n",
    "    nb.save(map2nifti(sl_res, sl_res.fa.clusters_featurewise_thresh),\n",
    "            opj(result_path, 'grp_03_clusters_labels_%s.nii.gz' % identifier))\n",
    "\n",
    "    # Same as above, but with accuracies instead of labels\n",
    "    file_id = opj(result_path, 'grp_03_clusters_acc_%s.nii.gz' % identifier)\n",
    "    nb.save(map2nifti(sl_res, np.array(\n",
    "        sl_res.fa.clusters_featurewise_thresh != 0).astype('int') * sl_res.samples), file_id)\n",
    "    run_atlasreader(file_id, atlasreader_names, atlasreader_prob_thresh)\n",
    "\n",
    "    # Store stats on clusters in CSV file\n",
    "    with open(opj(result_path, 'grp_03_stats_%s.csv' % identifier), 'w') as sFile:\n",
    "\n",
    "        header = [e[0] for e in sl_res.a.clusterstats.dtype.descr]\n",
    "        header += [e[0] for e in sl_res.a.clusterlocations.dtype.descr]\n",
    "\n",
    "        content = [','.join([str(e) for e in c] +\n",
    "                            ['(%s)' % ' '.join([str(l[0]) for l in sl_res.a.clusterlocations[i]]),\n",
    "                             '(%s)' % ' '.join([str(l[1]) for l in sl_res.a.clusterlocations[i]])])\n",
    "                   for i, c in enumerate(sl_res.a.clusterstats)]\n",
    "\n",
    "        sFile.write(','.join(header) + '\\n')\n",
    "        sFile.write('\\n'.join(content))\n",
    "\n",
    "    # Store cluster labels after FWE correction - largest cluster starts with `1`\n",
    "    if hasattr(sl_res.fa, 'clusters_fwe_thresh'):\n",
    "        nb.save(map2nifti(sl_res, sl_res.fa.clusters_fwe_thresh),\n",
    "                opj(result_path, 'grp_04_clusters_fwe_labels_%s.nii.gz' % identifier))\n",
    "\n",
    "        # Same as above, but with accuracies instead of labels\n",
    "        file_id = opj(result_path, 'grp_04_clusters_fwe_acc_%s.nii.gz' % identifier)\n",
    "        nb.save(map2nifti(sl_res, np.array(\n",
    "            sl_res.fa.clusters_fwe_thresh != 0).astype('int') * sl_res.samples), file_id)\n",
    "\n",
    "        # Plot results with atlasreader\n",
    "        run_atlasreader(file_id, atlasreader_names, atlasreader_prob_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to perform the group clustering threshold, according to Stelzer et al. (2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify order of parameter to iterate over\n",
    "if not session_list:\n",
    "    session_list = ['']\n",
    "    \n",
    "iteration_list = [[t, tf, sf, sess]\n",
    "                  for sess in session_list\n",
    "                  for tf in filters_temporal\n",
    "                  for sf in filters_spatial\n",
    "                  for t in tasks.keys()]\n",
    "iteration_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from nilearn.image import concat_imgs, math_img, iter_img\n",
    "from mvpa2.datasets.mri import map2nifti\n",
    "from mvpa2.algorithms.group_clusterthr import GroupClusterThreshold\n",
    "\n",
    "# Iterate over all parameters and run the searchlight analysis\n",
    "for task_id, tFilter, sFilter, session_id in iteration_list:\n",
    "\n",
    "    # Create filter idx\n",
    "    tFilter_id = '%s.%s' % (tFilter[0], tFilter[1])\n",
    "    sFilter_id = '%s.%s' % (sFilter[0], sFilter[1])\n",
    "\n",
    "    # Go through all targets\n",
    "    for targets in tasks[task_id]:\n",
    "        \n",
    "        # Specify classification folder\n",
    "        clf_folder = 'train-%s_test-%s' % (\n",
    "            '_'.join([str(s) for s in targets[0]]),\n",
    "            '_'.join([str(s) for s in targets[1]]))\n",
    "\n",
    "        # Go through specified classifiers\n",
    "        for clf_name in clf_names:\n",
    "\n",
    "            # Path to output folder\n",
    "            result_path = opj(out_path, 'task-%s' % task_id, 'group_stelzer', clf_folder)\n",
    "            if not os.path.exists(result_path):\n",
    "                os.makedirs(result_path)\n",
    "                \n",
    "            # Collect all files\n",
    "            file_idx = 'sub-*_tFilter-%s_sFilter-%s' % (\n",
    "                tFilter_id, sFilter_id)\n",
    "            file_idx += '_clf-%s_radius-%0d_steps-%03d.nii.gz' % (\n",
    "                clf_name, sphere_radius, sphere_steps)\n",
    "            path_orig = sorted(glob(opj(\n",
    "                out_path, 'task-%s' % task_id, clf_folder, file_idx)))\n",
    "            path_perm = sorted(glob(opj(\n",
    "                out_path, 'task-%s' % task_id, clf_folder, 'permutations',\n",
    "                file_idx.replace('.nii.gz', '_perm-*.nii.gz'))))\n",
    "\n",
    "            # Collect original and permutated searchlight results\n",
    "            run_identifer = '{} {} {} {} {} {}'.format(\n",
    "                task_id, tFilter, sFilter, session_id, clf_name, clf_folder)\n",
    "            print('Collecting subject specific data for: %s' % run_identifer)\n",
    "            imgs_orig = concat_imgs(path_orig)\n",
    "            imgs_perm = concat_imgs(path_perm)\n",
    "\n",
    "            # Compute group mask\n",
    "            group_mask = math_img(\n",
    "                'np.sum(img!=0, axis=-1)==%s' % imgs_orig.shape[-1], img=imgs_orig)\n",
    "            data_mask = group_mask.get_data()\n",
    "\n",
    "            # Extract accuracies\n",
    "            acc_orig = np.array([i.get_data()[data_mask!=0] for i in iter_img(imgs_orig)])\n",
    "            acc_perm = np.array([i.get_data()[data_mask!=0] for i in iter_img(imgs_perm)])\n",
    "\n",
    "            # Create mvpa datasets for group analysis\n",
    "            print('Creating datasets for: %s' % run_identifer)\n",
    "            ds_group = fmri_dataset(imgs_orig, mask=group_mask)\n",
    "            ds_orig = Dataset(acc_orig,\n",
    "                              sa=dict(subj=subject_list),\n",
    "                              a=ds_group.a)\n",
    "            perm_idx = [p[p.find('/sub-')+5:p.find('/sub-')+7] for p in path_perm]\n",
    "            ds_perm = Dataset(np.vstack((acc_orig, acc_perm)),\n",
    "                              sa=dict(subj=np.hstack((subject_list, perm_idx))),\n",
    "                              a=ds_group.a)\n",
    "            \n",
    "            # Perform Stelzer's group clustering threshold approach\n",
    "            print('Running Stelzer analysis for: %s' % run_identifer)\n",
    "            sl_res = run_stelzer(ds_orig, ds_perm, n_bootstrap, block_size, threshold_group,\n",
    "                                 multicomp_correction, fwe_rate, verbose=True)\n",
    "            \n",
    "            # Store outputs\n",
    "            print('Saving output for: %s' % run_identifer)\n",
    "            store_stelzer_results(\n",
    "                sl_res, n_bootstrap, threshold_group, multicomp_correction, fwe_rate,\n",
    "                file_idx, result_path, atlasreader_names, atlasreader_prob_thresh)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:mvpa]",
   "language": "python",
   "name": "conda-env-mvpa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
