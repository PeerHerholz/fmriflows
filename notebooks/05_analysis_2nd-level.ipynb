{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <p style=\"float: right;\"><img width=\"66%\" src=\"templates/logo_fmriflows.gif\"></p>\n",
    "    <h1>2nd-level Analysis</h1>\n",
    "    <p>This notebook performes the 2nd-level analysis in template space by executing the following steps:\n",
    "\n",
    "1. Specify and estimate 2nd-level one-sample-t-test\n",
    "2. Threshold contrasts\n",
    "3. Plot output\n",
    "\n",
    "**Note:** This notebook requires that the 1st-level analysis pipeline was already executed and that it's output can be found in the dataset folder under `/dataset/derivatives/fmriflows/analysis_1stLevel/univariate`. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure Requirements\n",
    "\n",
    "The data structure to run this notebook should be according to the BIDS format:\n",
    "\n",
    "    dataset\n",
    "    ├── fmriflows_spec_analysis.json\n",
    "    └── derivatives\n",
    "        └── fmriflows\n",
    "            └── analysis_1stLevel\n",
    "                └── univariate\n",
    "                    └── sub-{sub_id}\n",
    "                        └── task-{task_id}\n",
    "                            └── tFilter-{tFilter_id}_sFilter-{sFilter_id}\n",
    "                                └── con_[con_id]_norm.nii.gz\n",
    "\n",
    "`fmriflows` will run a one-sample-t-test 2nd-level analysis on all contrasts individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Specifications\n",
    "\n",
    "This notebook will extract the relevant analysis specifications from the `fmriflows_spec_analysis.json` file in the dataset folder. In the current setup, they are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join as opj\n",
    "\n",
    "spec_file = opj('/data', 'fmriflows_spec_analysis.json')\n",
    "\n",
    "with open(spec_file) as f:\n",
    "    specs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters for 1st-level analysis workflow\n",
    "subject_list = specs['subject_list']\n",
    "session_list = specs['session_list']\n",
    "tasks = specs['tasks']\n",
    "filters_spatial = specs['filters_spatial']\n",
    "filters_temporal = specs['filters_temporal']\n",
    "postfix = specs['analysis_postfix']\n",
    "gm_mask_thr = specs['gm_mask_thr']\n",
    "height_threshold = specs['height_threshold']\n",
    "use_fwe_correction = specs['use_fwe_correction']\n",
    "extent_threshold = specs['extent_threshold']\n",
    "use_topo_fdr = specs['use_topo_fdr']\n",
    "extent_fdr_p_threshold = specs['extent_fdr_p_threshold']\n",
    "atlasreader_names = specs['atlasreader_names']\n",
    "atlasreader_prob_thresh = specs['atlasreader_prob_thresh']\n",
    "n_proc = specs['n_parallel_jobs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to change any of those values manually, overwrite them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subject identifiers\n",
    "subject_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of session identifiers\n",
    "session_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of spatial filters (smoothing) that were used during functional preprocessing\n",
    "filters_spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of temporal filters that were used during functional preprocessing\n",
    "filters_temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a particular analysis postfix\n",
    "postfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value to threshold gray matter probability template to create 2nd-level mask\n",
    "gm_mask_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value for initial thresholding to define clusters\n",
    "height_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to use FWE (Bonferroni) correction for initial threshold\n",
    "use_fwe_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum cluster size in voxels \n",
    "extent_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to use FDR correction over cluster extent probabilities\n",
    "use_topo_fdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P threshold to use to on FDR corrected cluster size probabilities\n",
    "extent_fdr_p_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of atlases to use for creation of output tables\n",
    "atlasreader_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability threshold to use for output tables\n",
    "atlasreader_prob_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of parallel jobs to run\n",
    "n_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task specific parameters\n",
    "tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Workflow\n",
    "\n",
    "To ensure a good overview of the 1st-level analysis, the workflow was divided into an analysis and a report subworkflow.\n",
    "\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as opj\n",
    "from nipype import Node, MapNode, Workflow\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.algorithms.misc import Gunzip\n",
    "from nipype.interfaces.spm import OneSampleTTestDesign, EstimateModel, EstimateContrast, Threshold\n",
    "from nipype.interfaces.io import SelectFiles, DataSink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify SPM location\n",
    "from nipype.interfaces.matlab import MatlabCommand\n",
    "MatlabCommand.set_default_paths('/opt/spm12-r7219/spm12_mcr/spm12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Execution Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths and names\n",
    "exp_dir = '/data/derivatives'\n",
    "out_dir = 'fmriflows'\n",
    "work_dir = '/workingdir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Nodes for the 2nd-level Analysis Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Mask for group analysis\n",
    "def create_mask(mask_file, con_list, gm_mask_thr):\n",
    "\n",
    "    from os.path import abspath\n",
    "    from nilearn.image import resample_to_img, math_img, new_img_like\n",
    "    from scipy.ndimage.morphology import binary_dilation\n",
    "\n",
    "    # Resample mask image to contrast space and rescale to range of [0, 1]\n",
    "    img_mask = resample_to_img(mask_file, con_list[0])\n",
    "    mask = math_img('img/np.max(img) >= {}'.format(gm_mask_thr), img=img_mask).get_data()\n",
    "\n",
    "    # Apply binary dilation to image\n",
    "    mask = binary_dilation(mask, iterations=2)\n",
    "    img_mask = new_img_like(img_mask, mask, img_mask.affine)\n",
    "\n",
    "    # Save image as a NIfTI file\n",
    "    out_file = abspath('group_mask.nii')\n",
    "    img_mask.to_filename(out_file)\n",
    "   \n",
    "    return out_file\n",
    "    \n",
    "group_mask = Node(Function(input_names=['mask_file', 'con_list', 'gm_mask_thr'],\n",
    "                          output_names=['out_file'],\n",
    "                          function=create_mask),\n",
    "                 name='group_mask')\n",
    "group_mask.inputs.mask_file = '/templates/mni_icbm152_nlin_asym_09c/1.0mm_tpm_gm.nii.gz'\n",
    "group_mask.inputs.gm_mask_thr = gm_mask_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gunzip NIfTI files for SPM\n",
    "gunzip = MapNode(Gunzip(), name='gunzip', iterfield=['in_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2nd-level desing\n",
    "one_sample_ttest = Node(OneSampleTTestDesign(),\n",
    "                        name=\"one_sample_ttest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate 2nd-level model\n",
    "level2_estimate = Node(EstimateModel(estimation_method={'Classical': 1}),\n",
    "                       name=\"level2_estimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate 2nd-level contrasts\n",
    "level2_con_est = Node(EstimateContrast(group_contrast=True),\n",
    "                      name=\"level2_con_est\")\n",
    "\n",
    "cont01 = ['Group', 'T', ['mean'], [1]]\n",
    "level2_con_est.inputs.contrasts = [cont01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare contrast for two-tailed thresholding\n",
    "def abs_img(spmT_file):\n",
    "\n",
    "    from os.path import basename, abspath\n",
    "    from nilearn.image import math_img\n",
    "\n",
    "    img = math_img('np.abs(img)', img=spmT_file)\n",
    "    out_file = abspath(basename(spmT_file))\n",
    "    img.to_filename(out_file)\n",
    "\n",
    "    return out_file\n",
    "    \n",
    "absolute_image = Node(Function(input_names=['spmT_file'],\n",
    "                               output_names=['out_file'],\n",
    "                               function=abs_img),\n",
    "                      name='absolute_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold group contrast, voxel and cluster-wise\n",
    "threshold = Node(Threshold(contrast_index=1,\n",
    "                           use_topo_fdr=use_topo_fdr,\n",
    "                           use_fwe_correction=use_fwe_correction,\n",
    "                           extent_threshold=extent_threshold,\n",
    "                           height_threshold=height_threshold,\n",
    "                           height_threshold_type='p-value',\n",
    "                           extent_fdr_p_threshold=extent_fdr_p_threshold,\n",
    "                          ),\n",
    "                 name='threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask original spmT contrast with thresholded output\n",
    "def mask_img(spmT_file, thresh_image):\n",
    "\n",
    "    from os.path import basename, abspath\n",
    "    from nilearn.image import math_img\n",
    "\n",
    "    img = math_img('img * (np.nan_to_num(thr)!=0)', img=spmT_file, thr=thresh_image)\n",
    "    out_file = abspath(basename(spmT_file)).replace('.nii', '_thr.nii')\n",
    "    img.to_filename(out_file)\n",
    "\n",
    "    return out_file\n",
    "    \n",
    "apply_threshold = Node(Function(input_names=['spmT_file', 'thresh_image'],\n",
    "                                output_names=['out_file'],\n",
    "                                function=mask_img),\n",
    "                       name='apply_threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_atlasreader(thresh_image, atlas_names, atlas_prob_thresh, extent_threshold):\n",
    "\n",
    "    # Create output with atlasreader\n",
    "    from atlasreader.atlasreader import create_output\n",
    "    create_output(thresh_image,\n",
    "                  cluster_extent=extent_threshold,\n",
    "                  atlas=atlas_names,\n",
    "                  voxel_thresh=0,\n",
    "                  prob_thresh=atlas_prob_thresh)\n",
    "\n",
    "    # Collect atlasreader output files\n",
    "    from glob import glob\n",
    "    out_files = glob(thresh_image.replace('_thr.nii', '_thr*.png'))\n",
    "    out_files += glob(thresh_image.replace('_thr.nii', '_thr*.csv'))\n",
    "    \n",
    "    return out_files\n",
    "\n",
    "atlasreader = Node(Function(input_names=['thresh_image', 'atlas_names',\n",
    "                                         'atlas_prob_thresh', 'extent_threshold'],\n",
    "                              output_names=['out_files'],\n",
    "                              function=apply_atlasreader),\n",
    "                     name='atlasreader')\n",
    "atlasreader.inputs.atlas_names = atlasreader_names\n",
    "atlasreader.inputs.atlas_prob_thresh = atlasreader_prob_thresh\n",
    "atlasreader.inputs.extent_threshold = extent_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 2nd-level Analysis Workflow and connect nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analysis workflow\n",
    "analysis_2nd = Workflow(name='analysis_2nd')\n",
    "analysis_2nd.base_dir = work_dir\n",
    "\n",
    "# Add nodes to workflow and connect them\n",
    "analysis_2nd.connect([(gunzip, one_sample_ttest, [('out_file', 'in_files')]),\n",
    "                      (gunzip, group_mask, [('out_file', 'con_list')]),\n",
    "                      (group_mask, one_sample_ttest, [('out_file', 'explicit_mask_file')]),\n",
    "                      (one_sample_ttest, level2_estimate, [('spm_mat_file', 'spm_mat_file')]),\n",
    "                      (level2_estimate, level2_con_est, [('spm_mat_file', 'spm_mat_file'),\n",
    "                                                         ('beta_images', 'beta_images'),\n",
    "                                                         ('residual_image', 'residual_image')]),\n",
    "                      (level2_con_est, absolute_image, [('spmT_images', 'spmT_file')]),\n",
    "                      (level2_con_est, threshold, [('spm_mat_file', 'spm_mat_file')]),\n",
    "                      (absolute_image, threshold, [('out_file', 'stat_image')]),\n",
    "                      (level2_con_est, apply_threshold, [('spmT_images', 'spmT_file')]),\n",
    "                      (threshold, apply_threshold, [('thresholded_map', 'thresh_image')]),\n",
    "                      (apply_threshold, atlasreader, [('out_file', 'thresh_image')]),\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Input & Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over subject, session, task and run id\n",
    "info_source = Node(IdentityInterface(fields=['session_id',\n",
    "                                             'task_info',\n",
    "                                             'spatial_filt',\n",
    "                                             'temporal_filt']),\n",
    "                   name='info_source')\n",
    "\n",
    "# Generate a list of all possible contrasts, containing: (task_id, con_id, con_name)\n",
    "task_info = [(t, i, c[0]) for t in list(tasks.keys())\n",
    "                          for i, c in enumerate(tasks[t]['contrasts'])]\n",
    "\n",
    "# Combine all lists of iterations\n",
    "iter_list = [('task_info', task_info),\n",
    "             ('spatial_filt', filters_spatial),\n",
    "             ('temporal_filt', filters_temporal),\n",
    "             ]\n",
    "\n",
    "if session_list:\n",
    "    iter_list.append(('session_id', session_list))\n",
    "else:\n",
    "    info_source.inputs.session_id = ''\n",
    "\n",
    "info_source.iterables = iter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract contrast specifications for 2nd-level analysis\n",
    "def get_parameters(task_info):\n",
    "    \n",
    "    # Extract task information from (task_id, con_id)\n",
    "    task_id = task_info[0]\n",
    "    con_id = task_info[1] + 1\n",
    "\n",
    "    return task_id, con_id\n",
    "\n",
    "get_param = Node(Function(input_names=['task_info'],\n",
    "                          output_names=['task_id', 'con_id'],\n",
    "                          function=get_parameters),\n",
    "                 name='get_param')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path to input files\n",
    "def create_file_path(subject_list, session_id, task_id, tFilter, sFilter, con_id, postfix):\n",
    "\n",
    "    from glob import glob\n",
    "    template_con = '/data/derivatives/fmriflows/analysis_1stLevel'\n",
    "    if postfix:\n",
    "        template_con += '_%s' % postfix\n",
    "    template_con += '/univariate/sub-{0}/task-{1}/'\n",
    "    if session_id:\n",
    "        template_con += 'ses-%s/' % session_id\n",
    "    template_con += '{2}_{3}/con_{4}_norm.nii???'\n",
    "    \n",
    "    tFilter_id = 'tFilter_%s.%s' % (tFilter[0], tFilter[1])\n",
    "    sFilter_id = 'sFilter_%s.%s' % (sFilter[0], sFilter[1])\n",
    "\n",
    "    from glob import glob\n",
    "    con_files = []\n",
    "    for sub_id in subject_list:\n",
    "        new_cons = glob(template_con.format(\n",
    "            sub_id, task_id, tFilter_id, sFilter_id, '%04d' % con_id))\n",
    "        con_files += new_cons\n",
    "\n",
    "    return sorted(con_files)\n",
    "\n",
    "select_files = Node(Function(input_names=['subject_list', 'session_id', 'task_id',\n",
    "                                          'tFilter', 'sFilter', 'con_id', 'postfix'],\n",
    "                             output_names=['con_files'],\n",
    "                             function=create_file_path),\n",
    "                    name='select_files')\n",
    "select_files.inputs.subject_list = subject_list\n",
    "select_files.inputs.postfix = postfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save relevant outputs in a datasink\n",
    "datasink = Node(DataSink(base_directory=exp_dir,\n",
    "                         container=out_dir),\n",
    "                name='datasink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the following naming substitutions for the datasink\n",
    "if session_list:\n",
    "\n",
    "    folder_old = ['_session_id_%s_spatial_filt_%s_task_info_%s_temporal_filt_%s/' % (\n",
    "        ses, '.'.join([str(f) for f in sFilter]),\n",
    "        '.'.join([str(t) for t in task]),\n",
    "        '.'.join([str(t) for t in tFilter]))\n",
    "                  for ses in session_list\n",
    "                  for task in task_info\n",
    "                  for sFilter in filters_spatial\n",
    "                  for tFilter in filters_temporal]\n",
    "\n",
    "    folder_new = ['task-%s/ses-%s/tFilter_%s_sFilter_%s/' % (\n",
    "        '{}/{}'.format(task[0], task[2]),\n",
    "        ses,\n",
    "        '.'.join([str(t) for t in tFilter]),\n",
    "        '.'.join([str(f) for f in sFilter]))\n",
    "                  for task in task_info\n",
    "                  for ses in session_list\n",
    "                  for sFilter in filters_spatial\n",
    "                  for tFilter in filters_temporal]\n",
    "else:\n",
    "    \n",
    "    folder_old = ['_spatial_filt_%s_task_info_%s_temporal_filt_%s/' % (\n",
    "        '.'.join([str(f) for f in sFilter]),\n",
    "        '.'.join([str(t) for t in task]),\n",
    "        '.'.join([str(t) for t in tFilter]))\n",
    "                  for task in task_info\n",
    "                  for sFilter in filters_spatial\n",
    "                  for tFilter in filters_temporal]\n",
    "\n",
    "    folder_new = ['task-%s/tFilter_%s_sFilter_%s/' % (\n",
    "        '{}/{}'.format(task[0], task[2]),\n",
    "        '.'.join([str(t) for t in tFilter]),\n",
    "        '.'.join([str(f) for f in sFilter]))\n",
    "                  for task in task_info\n",
    "                  for sFilter in filters_spatial\n",
    "                  for tFilter in filters_temporal]\n",
    "    \n",
    "substitutions = [z for z in zip(folder_old, folder_new)]\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Input & Output Stream to 2nd-Level Analysis Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create anatomical preprocessing workflow\n",
    "out_folder = 'analysis_2ndLevel'\n",
    "if postfix:\n",
    "    out_folder += '_%s' % postfix\n",
    "\n",
    "# Add nodes to workflow and connect them\n",
    "analysis_2nd.connect([(info_source, get_param, [('task_info', 'task_info')]),\n",
    "                      (info_source, select_files, [('session_id', 'session_id'),\n",
    "                                                   ('spatial_filt', 'sFilter'),\n",
    "                                                   ('temporal_filt', 'tFilter')]),\n",
    "                      (get_param, select_files, [('task_id', 'task_id')]),\n",
    "                      (get_param, select_files, [('con_id', 'con_id')]),\n",
    "                      \n",
    "                      (select_files, gunzip, [('con_files', 'in_file')]),\n",
    "                      \n",
    "                      # Store analysis results in datasink\n",
    "                      (level2_estimate, datasink, [('beta_images', '%s.univariate.@betas' % out_folder),\n",
    "                                                   ('residual_image', '%s.univariate.@ResMS' % out_folder)]),\n",
    "                      (level2_con_est, datasink, [('spm_mat_file', '%s.univariate.@spm_mat' % out_folder),\n",
    "                                                  ('con_images', '%s.univariate.@con' % out_folder),\n",
    "                                                  ('ess_images', '%s.univariate.@ess' % out_folder),\n",
    "                                                  ('spmT_images', '%s.univariate.@spmT' % out_folder),\n",
    "                                                  ('spmF_images', '%s.univariate.@spmF' % out_folder)]),\n",
    "                      (apply_threshold, datasink, [('out_file', '%s.univariate.@thr_con' % out_folder)]),\n",
    "                      (atlasreader, datasink, [('out_files', '%s.univariate.@atlas_files' % out_folder)]),\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create analysis_1st output graph\n",
    "analysis_2nd.write_graph(graph2use='colored', format='png', simple_form=True)\n",
    "\n",
    "# Visualize the graph in the notebook\n",
    "from IPython.display import Image\n",
    "Image(filename=opj(analysis_2nd.base_dir, 'analysis_2nd', 'graph.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the workflow in parallel mode\n",
    "res = analysis_2nd.run(plugin='MultiProc', plugin_args={'n_procs' : n_proc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save workflow graph visualizations in datasink\n",
    "analysis_2nd.write_graph(graph2use='flat', format='png', simple_form=True)\n",
    "analysis_2nd.write_graph(graph2use='colored', format='png', simple_form=True)\n",
    "\n",
    "from shutil import copyfile\n",
    "copyfile(opj(analysis_2nd.base_dir, 'analysis_2nd', 'graph.png'),\n",
    "         opj(exp_dir, out_dir,  out_folder, 'graph.png'))\n",
    "copyfile(opj(analysis_2nd.base_dir, 'analysis_2nd', 'graph_detailed.png'),\n",
    "         opj(exp_dir, out_dir, out_folder, 'graph_detailed.png'));"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
